{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from numpy import loadtxt, zeros, ones, array, linspace, logspace\n",
    "from pylab import scatter, show, title, xlabel, ylabel, plot, contour\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cross_validation, metrics\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeter Data Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "\n",
    "consumer_key = ' '\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "\n",
    "class twitterListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print (data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (status)\n",
    "\n",
    "\n",
    "l = twitterListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "stream = Stream(auth, l)\n",
    "\n",
    "stream.filter(languages=[\"en\"], track=['medicine', 'meds', 'medical', 'infected', 'health', 'pill', 'pills', 'drug', 'drugs',\\\n",
    "                                        'surgeon', 'surgeons', 'disease', 'diseases', 'virus', 'viruses', 'bacteria',\\\n",
    "                                        'vaccine', 'vaccines', 'doctor', 'doctors', 'patient', 'patients','hospital'\\\n",
    "                                        'clinic', 'prescription', 'sick', 'illness', 'allergy', 'fever', 'flu', 'autism'\\\n",
    "                                        'cancer', 'diabetes', 'HIV', 'injection', 'antibiotic', 'disorder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Tweets Information into CSV and Extract from json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def readFromFile(filePath):\n",
    "    # Read from the file and append the data to a list\n",
    "\n",
    "    tweets_data_path = filePath\n",
    "    tweets_data = []\n",
    "    tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "    for line in tweets_file:\n",
    "        # append json file data to a list\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return tweets_data\n",
    "    \n",
    "\n",
    "def listToDataFrame(lst, headerList):\n",
    "    # Read from a list and create a Pandas dataframe with the corresponding headers \n",
    "    return pd.DataFrame(lst, columns=header)\n",
    "\n",
    "\n",
    "header = ['text', 'created_at', 'user']\n",
    "filePath = r'C:\\Users\\twittermed.txt'\n",
    "\n",
    "#print(readFromFile(filePath)[:5])\n",
    "dataList = readFromFile(filePath)\n",
    "tweets = listToDataFrame(dataList, header)\n",
    "tweets['is_retweeted'] = [True if 'retweeted_status' in d else False for d in dataList]\n",
    "#tweets['bio'] = [d.get('description', False) for d in list(tweets.user)]\n",
    "#[True if 9 in i else False for i in a]\n",
    "#print(tweets.head())\n",
    "print(len(tweets))\n",
    "\n",
    "tweets = tweets.dropna(axis=0, how='any') #delete NAN rows\n",
    "print(len(tweets))\n",
    "\n",
    "tweets['screen_name'] = [d.get('screen_name') for d in list(tweets.user)]\n",
    "tweets['followers_count'] = [d.get('followers_count') for d in list(tweets.user)]\n",
    "tweets['friends_count'] = [d.get('friends_count') for d in list(tweets.user)]\n",
    "tweets['statuses_count'] = [d.get('statuses_count') for d in list(tweets.user)]\n",
    "tweets['favourites_count'] = [d.get('favourites_count') for d in list(tweets.user)]\n",
    "tweets['user_created_at'] = [d.get('created_at') for d in list(tweets.user)]\n",
    "tweets['bio'] = [d.get('description') for d in list(tweets.user)]\n",
    "tweets = tweets[tweets.is_retweeted == False]\n",
    "\n",
    "\n",
    "#tweets['verified'] = [d.get('verified') for d in list(tweets.user)]\n",
    "#tweets = tweets.dropna(axis=0, how='any') #delete NAN rows\n",
    "print(tweets.shape)\n",
    "\n",
    "tweets.to_csv(\"twittertest2.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV file (extracted information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "tweets = pd.read_csv(\"twittertest2.csv\", encoding = \"UTF-8\", engine='python')\n",
    "############# remove unwanted tweets\n",
    "tweets.head()\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove tweets which has Null Bio values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets.dropna(axis=0, how='any')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Twitter User Bio or Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def addd(x): \n",
    "    #print(x['bio'])\n",
    "    x['bio'] = x['bio'].lower()\n",
    "    return re.sub ('[^a-zA-Z]', ' ', x['bio'])\n",
    "\n",
    "tweets['bio'] = tweets.apply(addd, 1)\n",
    "tweets = tweets.dropna(axis=0, how='any')\n",
    "print(tweets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Data Preprosseing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#tweets = pd.read_csv(\"twittertest2.csv\",  encoding = \"ISO-8859-1\", header = 0, engine='python')\n",
    "\n",
    "\n",
    "############# remove unwanted tweets\n",
    "\n",
    "#### Remove RTs:\n",
    "print(len(tweets))\n",
    "\n",
    "#print(len(tweets))\n",
    "#tweets = tweets[tweets.is_retweeted == False]\n",
    "print(len(tweets))\n",
    "\n",
    "#### Check the membership time\n",
    "\n",
    "def __datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%a %b %d %H:%M:%S +0000 %Y')\n",
    "\n",
    "def apply_on_df(x):\n",
    "    return (__datetime(x['created_at']) - __datetime(x['user_created_at'])).days\n",
    "    \n",
    "\n",
    "tweets['active_days'] = tweets.apply(apply_on_df, axis = 1)\n",
    "#print(tweets.head())\n",
    "\n",
    "#print(len(tweets))\n",
    "tweets = tweets[tweets.active_days > 90 ]\n",
    "#print(len(tweets))\n",
    "\n",
    "\n",
    "#### Check friends and following\n",
    "\n",
    "#print(len(tweets))\n",
    "tweets = tweets[tweets.followers_count - tweets.friends_count < 700]\n",
    "#print(len(tweets))\n",
    "\n",
    "##### Remove bots\n",
    "\n",
    "#print(len(tweets))\n",
    "tweets = tweets[tweets.statuses_count // tweets.active_days < 15]\n",
    "print(len(tweets))\n",
    "print(tweets.shape)\n",
    "\n",
    "\n",
    "###### Check for wanted tweets\n",
    "\n",
    "#keywordList = ['medical', 'physician' , 'doctor', 'nurse']\n",
    "#tweets['possible'] = tweets[True if any(k in tweets.bio) else False for k in keywordList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter User who has valid Bio information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['bio'].str.contains(\"physician|hospital|doctor|clinic|nurse|specialist|gynecologist|dentist|medicine|surgeon\")]\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet2 = tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Previous tweets of Twitter User (Assumed Expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "\n",
    "access_token_secret = ''\n",
    "\n",
    "\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_or_store(tweet):\n",
    "    print(json.dumps(tweet))\n",
    "    \n",
    "    \n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def checkStatus(x):\n",
    "    try:\n",
    "        status = []\n",
    "        dictioanry = {}\n",
    "        stuff = api.user_timeline(x['screen_name'], count = 10, include_rts = False)\n",
    "        for s in stuff:\n",
    "            if s.str.contains(\"medicine|meds|medical|infected|health|pill|pills|drug|drugs|\\\n",
    "                            surgeon|surgeons|disease|diseases|virus|viruses|bacteria|vaccine|\\\n",
    "                            vaccines|doctor|doctors|patient|patients|hospital|clinic|prescription|\\\n",
    "                            sick|illness|allergy|fever|flu|autism|cancer|diabetes|HIV|injection|\\\n",
    "                            antibiotic|disorder|physician|dentist|gynecologist|surgeon\"):\n",
    "                 status.append(s.text)\n",
    "        return status\n",
    "    except:\n",
    "        print(\"Failed to run the command on that user, Skipping...\") \n",
    "        \n",
    "tweet2['statuses'] = tweet2.apply(checkStatus, axis = 1)\n",
    "\n",
    "tweet2.to_csv(\"twitter_first_result_status_xx.csv\",index=False, encoding = 'utf-8', columns = ['screen_name', 'statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets3 = pd.read_csv(\"twitter_first_result_status_xx.csv\", encoding = \"UTF-8\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Non Medical Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = \"\" # Use your own key. To get a key https://apps.twitter.com/\n",
    "consumer_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key=consumer_key, consumer_secret=consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for tweet in tweepy.Cursor(api.search, q='\"job\" OR \"Hiring\" OR \"sports\" OR \"play\" OR \"rain\" OR \"book\"').items(400):\n",
    "    results.append(tweet)\n",
    "\n",
    "print( len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_results(results):\n",
    "    id_list = [tweet.id for tweet in results]\n",
    "    data_set = pd.DataFrame(id_list, columns=[\"id\"])\n",
    "\n",
    "    # Processing Tweet Data\n",
    "\n",
    "    data_set[\"text\"] = [tweet.text for tweet in results]\n",
    "    data_set[\"created_at\"] = [tweet.created_at for tweet in results]\n",
    "    data_set[\"retweet_count\"] = [tweet.retweet_count for tweet in results]\n",
    "    #data_set[\"retweeted_status\"] = [tweet.retweet_status for tweet in results]\n",
    "    data_set[\"favorite_count\"] = [tweet.favorite_count for tweet in results]\n",
    "    data_set[\"source\"] = [tweet.source for tweet in results]\n",
    "\n",
    "    # Processing User Data\n",
    "    data_set[\"user_id\"] = [tweet.author.id for tweet in results]\n",
    "    data_set[\"screen_name\"] = [tweet.author.screen_name for tweet in results]\n",
    "    data_set[\"user_name\"] = [tweet.author.name for tweet in results]\n",
    "    data_set[\"user_created_at\"] = [tweet.author.created_at for tweet in results]\n",
    "    data_set[\"user_description\"] = [tweet.author.description for tweet in results]\n",
    "    data_set[\"user_followers_count\"] = [tweet.author.followers_count for tweet in results]\n",
    "    data_set[\"user_friends_count\"] = [tweet.author.friends_count for tweet in results]\n",
    "    data_set[\"user_location\"] = [tweet.author.location for tweet in results]\n",
    "\n",
    "    return data_set\n",
    "data_set = process_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set.to_csv(\"nonmedical.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_medical_tweets = pd.read_csv(\"nonmedical.csv\", encoding = \"UTF-8\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_medical_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "\n",
    "access_token_secret = ''\n",
    "\n",
    "\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_or_store(tweet):\n",
    "    print(json.dumps(tweet))\n",
    "    \n",
    "    \n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#user = api.get_user('EarlofNarwhals')\n",
    "\n",
    "#for friend in tweepy.Cursor(user.friends).items():\n",
    "#    process_or_store(friend._json)\n",
    "\n",
    "\n",
    "### extract the 5 recent statuses of each user\n",
    "def checkStatus(x):\n",
    "    try:\n",
    "        status = []\n",
    "        dictioanry = {}\n",
    "        stuff = api.user_timeline(x['screen_name'], count = 5, include_rts = False)\n",
    "        for s in stuff:\n",
    "            #if s.str.contains(\"medicine|meds|medical|infected|health|pill|pills|drug|drugs|surgeon|surgeons|disease|diseases|virus|viruses|bacteria|vaccine|vaccines|doctor|doctors|patient|patients|hospital|clinic|prescription|sick|illness|allergy|fever|flu|autism|cancer|diabetes|HIV|injection|antibiotic|disorder|physician|dentist|gynecologist|surgeon\"):\n",
    "             status.append(s.text)\n",
    "        #dictioanry['sts'] = status\n",
    "        #return dictioanry\n",
    "        return status\n",
    "    except:\n",
    "        print(\"Failed to run the command on that user, Skipping...\")\n",
    "        \n",
    "\n",
    "\n",
    "non_medical_tweets['statuses'] = non_medical_tweets.apply(checkStatus, axis = 1)\n",
    "\n",
    "non_medical_tweets.to_csv(\"twitter_nonexpert_result_status_xx.csv\",index=False, encoding = 'utf-8', columns = ['screen_name', 'statuses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_medical_tweets2 = pd.read_csv(\"twitter_nonexpert_result_status_xx.csv\", encoding = \"UTF-8\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(non_medical_tweets2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Expert and Non Expert Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expert = pd.read_csv(\"projectData/data/twitter_expert.csv\", encoding = \"UTF-8\", engine='python')\n",
    "non_expert = pd.read_csv(\"projectData/data/twitter_nonexpert.csv\", encoding = \"UTF-8\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(expert.shape)\n",
    "print(non_expert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = [expert, non_expert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_tweets =pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(total_tweets.shape)\n",
    "total_tweets.head()\n",
    "#total_tweets.columns[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "total_tweets = shuffle(total_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet_status Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "#df = df.dropna(how='any',axis=0)\n",
    "title_doc = total_tweets[total_tweets.columns[1]].values\n",
    "corpus =[]\n",
    "for row in title_doc:\n",
    "    #remove unwanted data\n",
    "    title = re.sub ('[^a-zA-Z]', ' ', row)\n",
    "    title = title.lower()   \n",
    "    title = title.split()\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    #word stemming\n",
    "    title = [lemma.lemmatize(item) for item in title if item not in stop]\n",
    "    title = ' '.join(title)\n",
    "    corpus.append(title)\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Term Matrix for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(corpus)\n",
    "# X = np.array(X)\n",
    "#data_matrix = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "#print(data_matrix.shape)\n",
    "y= total_tweets.iloc[:, 2].values\n",
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the Dataset into 80% training & 20% testing - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# sv = svm.SVC()\n",
    "# clf_sv = sv.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf_rf = rf.fit(X_train,y_train)\n",
    "print (\"Acurracy: \", clf_rf.score(X_test,y_test))\n",
    "y_pred = clf_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=5, max_depth=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(random_forest_clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y, predictions, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute train and test errors\n",
    "alphas = np.logspace(-5, 1, 60)\n",
    "enet = linear_model.ElasticNet(l1_ratio=0.7)\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "for alpha in alphas:\n",
    "    enet.set_params(alpha=alpha)\n",
    "    enet.fit(X_train, y_train)\n",
    "    train_errors.append(enet.score(X_train, y_train))\n",
    "    test_errors.append(enet.score(X_test, y_test))\n",
    "\n",
    "i_alpha_optim = np.argmax(test_errors)\n",
    "alpha_optim = alphas[i_alpha_optim]\n",
    "print(\"Optimal regularization parameter : %s\" % alpha_optim)\n",
    "\n",
    "# Estimate the coef_ on full data with optimal regularization parameter\n",
    "enet.set_params(alpha=alpha_optim)\n",
    "coef_ = enet.fit(X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(alphas, train_errors, label='Train error')\n",
    "plt.semilogx(alphas, test_errors, label='Test error')\n",
    "plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n",
    "           linewidth=3, label='Optimum on test')\n",
    "plt.legend(loc='lower left')\n",
    "plt.ylim([0, 1.2])\n",
    "plt.xlabel('Regularization parameter')\n",
    "plt.ylabel('Performance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef = np.random.randn(500)\n",
    "# Show estimated coef_ vs true coef\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(coef, label='True coef')\n",
    "plt.plot(coef_, label='Estimated coef')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "\n",
    "X1 =X\n",
    "y1=y\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Random forest)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "plot_learning_curve(estimator, title, X1, y1, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "plot_learning_curve(estimator, title, X1, y1, (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
