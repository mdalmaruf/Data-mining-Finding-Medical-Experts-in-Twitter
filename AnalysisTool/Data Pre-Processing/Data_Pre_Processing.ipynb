{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from numpy import loadtxt, zeros, ones, array, linspace, logspace\n",
    "from pylab import scatter, show, title, xlabel, ylabel, plot, contour\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import cross_validation, metrics\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read .csv from provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>crawled</th>\n",
       "      <th>title</th>\n",
       "      <th>Original content</th>\n",
       "      <th>pageviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9546141</td>\n",
       "      <td>2016-01-01T04:47:00+00:00</td>\n",
       "      <td>Kanye West Releases Awful New Song on New Year...</td>\n",
       "      <td>Is Drake inside Kanye West??s head? That??s ...</td>\n",
       "      <td>29478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9544391</td>\n",
       "      <td>2016-01-01T05:01:00+00:00</td>\n",
       "      <td>Why Everybody Should Work in Hollywood</td>\n",
       "      <td>I??m convinced that working in Hollywood is t...</td>\n",
       "      <td>24939.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9544392</td>\n",
       "      <td>2016-01-01T05:01:00+00:00</td>\n",
       "      <td>Bill Cosby Tamir Rice And The Power of Prosecu...</td>\n",
       "      <td>What do Bill Cosby and Tamir Rice??s have in ...</td>\n",
       "      <td>13595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9544393</td>\n",
       "      <td>2016-01-01T05:01:00+00:00</td>\n",
       "      <td>Translating the ??Iliad??? Who Isn??t.</td>\n",
       "      <td>Pop quiz: Which is greater? (a) The number of ...</td>\n",
       "      <td>4752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9544394</td>\n",
       "      <td>2016-01-01T05:01:00+00:00</td>\n",
       "      <td>How Oprah Created a Profitable Weight-Loss Plan</td>\n",
       "      <td>Forget portion control: weight loss is a power...</td>\n",
       "      <td>26549.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                    crawled  \\\n",
       "0  9546141  2016-01-01T04:47:00+00:00   \n",
       "1  9544391  2016-01-01T05:01:00+00:00   \n",
       "2  9544392  2016-01-01T05:01:00+00:00   \n",
       "3  9544393  2016-01-01T05:01:00+00:00   \n",
       "4  9544394  2016-01-01T05:01:00+00:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Kanye West Releases Awful New Song on New Year...   \n",
       "1             Why Everybody Should Work in Hollywood   \n",
       "2  Bill Cosby Tamir Rice And The Power of Prosecu...   \n",
       "3          Translating the ??Iliad??? Who Isn??t.   \n",
       "4    How Oprah Created a Profitable Weight-Loss Plan   \n",
       "\n",
       "                                    Original content  pageviews  \n",
       "0  Is Drake inside Kanye West??s head? That??s ...    29478.0  \n",
       "1  I??m convinced that working in Hollywood is t...    24939.0  \n",
       "2  What do Bill Cosby and Tamir Rice??s have in ...    13595.0  \n",
       "3  Pop quiz: Which is greater? (a) The number of ...     4752.0  \n",
       "4  Forget portion control: weight loss is a power...    26549.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_filename=\"data_mining.csv\"\n",
    "df=pd.read_csv(csv_filename, encoding='ISO-8859-1')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing \n",
    "###  Replace Noisy Data, Remove Stopwords/Null values/Punctuations and Data stemming, lemmatize\n",
    "### 3.1 Title Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kanye west release awful new song new year eve', 'everybody work hollywood', 'bill cosby tamir rice power prosecutor', 'translating iliad', 'oprah created profitable weight loss plan']\n"
     ]
    }
   ],
   "source": [
    "#ps = PorterStemmer()\n",
    "#sno = nltk.stem.SnowballStemmer('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "df = df.dropna(how='any',axis=0)\n",
    "title_doc = df[df.columns[2]].values\n",
    "corpus =[]\n",
    "for row in title_doc:\n",
    "    #remove unwanted data\n",
    "    title = re.sub ('[^a-zA-Z]', ' ', row)\n",
    "    title = title.lower()   \n",
    "    title = title.split()\n",
    "    #remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    #word stemming\n",
    "    title = [lemma.lemmatize(item) for item in title if item not in stop]\n",
    "    title = ' '.join(title)\n",
    "    corpus.append(title)\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Content Part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_doc = df[df.columns[3]].values\n",
    "corpus2 =[]\n",
    "for i in content_doc:\n",
    "    content = re.sub ('[^a-zA-Z]', ' ', i)\n",
    "    content = content.lower()\n",
    "    content = content.split()\n",
    "    stop = stopwords.words('english')\n",
    "    ps = PorterStemmer()\n",
    "    content = [lemma.lemmatize(item) for item in content if item not in stop]\n",
    "    content = ' '.join(content)\n",
    "    corpus2.append(content)\n",
    "    \n",
    "#print(corpus2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Document Term Matrix for Title\n",
    "### 4.1 View first 4 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   awful  bill  cosby  eve  everybody  hollywood  iliad  kanye  new  power  \\\n",
      "0      1     0      0    1          0          0      0      1    2      0   \n",
      "1      0     0      0    0          1          1      0      0    0      0   \n",
      "2      0     1      1    0          0          0      0      0    0      1   \n",
      "3      0     0      0    0          0          0      1      0    0      0   \n",
      "\n",
      "   prosecutor  release  rice  song  tamir  translating  west  work  year  \n",
      "0           0        1     0     1      0            0     1     0     1  \n",
      "1           0        0     0     0      0            0     0     1     0  \n",
      "2           1        0     1     0      1            0     0     0     0  \n",
      "3           0        0     0     0      0            1     0     0     0  \n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "X_data_view = vec.fit_transform(corpus[:4])\n",
    "data_matrix = pd.DataFrame(X_data_view.toarray(), columns=vec.get_feature_names())\n",
    "print(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vector matrix for title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[ 1.  1.  0. ...,  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(corpus).toarray()\n",
    "print(X)\n",
    "\n",
    "# handle goal attrubte to binary \n",
    "# Catagorize the pageview count into popular and unpopular \n",
    "popular = df.pageviews >= 15000\n",
    "unpopular = df.pageviews < 15000\n",
    "df.loc[popular,'pageviews'] = 1\n",
    "df.loc[unpopular,'pageviews'] = 0\n",
    "y= df.iloc[:, 4].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Document Term Matrix for Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 View first 3 rows for content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  absolutely  accompaniment  accountability  accountable  \\\n",
      "0        0     0           0              1               0            0   \n",
      "1        1     2           1              0               1            0   \n",
      "2        0     0           0              0               0            1   \n",
      "\n",
      "   accusation  accused  act  actualized   ...     wrath  writer  xanax  year  \\\n",
      "0           0        0    0           0   ...         0       0      0     2   \n",
      "1           0        0    1           1   ...         1       3      1     2   \n",
      "2           1        2    0           0   ...         0       0      0     7   \n",
      "\n",
      "   yeezy  yeezys  yes  yet  young  zealous  \n",
      "0      8       1    0    1      0        0  \n",
      "1      0       0    2    0      1        0  \n",
      "2      0       0    0    0      1        1  \n",
      "\n",
      "[3 rows x 920 columns]\n"
     ]
    }
   ],
   "source": [
    "X_data_view2 = vec.fit_transform(corpus2[:3])\n",
    "data_matrix2 = pd.DataFrame(X_data_view2.toarray(), columns=vec.get_feature_names())\n",
    "print(data_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vector matrix for content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[ 1.  1.  0. ...,  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vec2 = CountVectorizer()\n",
    "X1 = vec.fit_transform(corpus2).toarray()\n",
    "print(X1)\n",
    "y1= df.iloc[:, 4].values\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Split the Dataset into 70% training & 30% testing - Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1018, 3386)\n",
      "(1018,)\n",
      "(437, 3386)\n",
      "(437,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "#print(df.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Model for Title\n",
    "#### To train the model, I divide the pageviews into two category- popular and unpopular (Binary Classification). I assume that if the page view is higher than 15000, then it is popular. Otherwise, it is unpopular.  \n",
    "### 7.1 GaussianNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Prediction and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.613272311213\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_pred, normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. Training Model for Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = cross_validation.train_test_split(X1, y1, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8.1 GaussianNB Model for Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.549199084668\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y1_pred = clf.predict(X1_test)\n",
    "confusion_matrix(y1_test, y1_pred)\n",
    "print(\"Accuracy\", accuracy_score(y1_test, y1_pred, normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried Different Classification Models \n",
    "### Decision Tree Model for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.572082379863\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "clf_dt = dt.fit(X_train, y_train)\n",
    "print (\"Acurracy: \", clf_dt.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest Model for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.562929061785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf_rf = rf.fit(X_train,y_train)\n",
    "print (\"Acurracy: \", clf_rf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.569794050343\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "sv = svm.SVC()\n",
    "clf_sv = sv.fit(X_train,y_train)\n",
    "print (\"Acurracy: \", clf_sv.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model for Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.62471395881\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "clf_dt = dt.fit(X1_train, y1_train)\n",
    "\n",
    "print (\"Acurracy: \", clf_dt.score(X1_test,y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model for content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.617848970252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf_rf = rf.fit(X1_train,y1_train)\n",
    "print (\"Acurracy: \", clf_rf.score(X1_test,y1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier for content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurracy:  0.606407322654\n"
     ]
    }
   ],
   "source": [
    "sv = svm.SVC()\n",
    "clf_sv = sv.fit(X1_train,y1_train)\n",
    "print (\"Acurracy: \", clf_sv.score(X1_test,y1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
