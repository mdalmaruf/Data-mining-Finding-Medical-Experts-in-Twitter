{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data =pd.read_csv('assignment3/kaggle/training.csv', header=None)\n",
    "training_data2 = pd.read_csv('assignment3/kaggle/training2.csv', header=None)\n",
    "training_label = pd.read_csv('assignment3/kaggle/ctrain.csv', header=None)\n",
    "test_data = pd.read_csv('assignment3/kaggle/test.csv', header=None)\n",
    "test_data2 = pd.read_csv('assignment3/kaggle/test2.csv', header=None)\n",
    "\n",
    "\n",
    "\n",
    "# training_data =open('assignment3/kaggle/training.csv')\n",
    "# training_label = open('assignment3/kaggle/ctrain.csv','r')\n",
    "# test_data = open('assignment3/kaggle/test.csv','r')\n",
    "# training_label = pd.DataFrame(columns=['label'], data = pd.read_csv('assignment3/kaggle/ctrain.csv',header=None).values)\n",
    "\n",
    "# print(pd.value_counts(training_label['label'].values))\n",
    "# print(training_label.head())\n",
    "#print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = training_data.values\n",
    "training2 = training_data2.values\n",
    "test = test_data.values\n",
    "test2 = test_data2.values\n",
    "ctrain = training_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 50218)\n",
      "(2388, 13384)\n",
      "(597, 50218)\n",
      "(597, 13384)\n",
      "(2388, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training.shape)\n",
    "print(training2.shape)\n",
    "print(test.shape)\n",
    "print(test2.shape)\n",
    "print(ctrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388,)\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(ctrain.ravel())\n",
    "encoded_Y = encoder.transform(ctrain.ravel())\n",
    "print(encoded_Y.shape)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "ytrain = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 63602)\n",
      "(597, 63602)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xtrain = np.column_stack((training, training2))\n",
    "xtest = np.column_stack((test, test2))\n",
    "# xtrain = training2\n",
    "# xtest = test_data2\n",
    "\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 63602)\n",
      "(597, 63602)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "#xtrain = tfidf_transformer.fit_transform(xtrain)\n",
    "#xtest = tfidf_transformer.fit_transform(xtest)\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1418\n",
       "2     547\n",
       "0     423\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(ctrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_to_file(output_matrix, name):\n",
    "    with open(name, 'w') as f:\n",
    "        idx = 1\n",
    "        f.write('id,class\\n')\n",
    "        for i in output_matrix:\n",
    "            f.write(str(idx) + ',' +str(i)+'\\n')    \n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bdt_discrete = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=600,\n",
    "    learning_rate=1.5,\n",
    "    algorithm=\"SAMME\")\n",
    "pred_d = bdt_discrete.fit(xtrain, ctrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dd = bdt_discrete.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 2 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 1 2 1 1 1 2 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 1 2\n",
      " 1 1 0 1 2 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1\n",
      " 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 0 2 0 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1\n",
      " 2 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2\n",
      " 2 2 1 2 1 1 1 2 1 1 1 1 0 2 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 1 1 0 2\n",
      " 1 1 1 1 1 1 0 2 1 1 2 1 1 1 1 1 1 1 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 2 1 0 2 2 1 1 0 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1\n",
      " 2 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    501\n",
       "2     82\n",
       "0     14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pred_dd)\n",
    "print_to_file(pred,'testing_kaggle/ada_boost_decision_tree.csv')\n",
    "pd.value_counts(pred_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(xtrain, ctrain.ravel())\n",
    "pred = clf.predict(xtest)\n",
    "print_to_file(pred,'testing_kaggle/ada_boost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 2 1 1 1 1 1 1 1 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 0 1 1\n",
      " 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 1 2\n",
      " 1 1 1 1 0 2 2 1 1 1 2 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 1 2\n",
      " 1 1 1 1 1 1 1 1 2 1 1 1 2 2 0 1 1 0 1 1 2 1 2 1 1 1 1 1 2 1 1 1 1 1 1 2 1\n",
      " 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 1 2 1 1 1 1 0 2 2 1 1 2 2 1 0 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 2 1 2 1 1 1 2 1 0 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1\n",
      " 1 2 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1\n",
      " 2 1 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 1 2 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 2 1 0 1 1 1 1 1\n",
      " 2 2 1 0 1 1 1 0 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 1 1 1 2\n",
      " 0 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 0 2 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 2 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 2 1 1\n",
      " 2 2 1 0 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1\n",
      " 2 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    482\n",
       "2     90\n",
       "0     25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(pred)\n",
    "pd.value_counts(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    512\n",
       "2     72\n",
       "0     13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clfDT = tree.DecisionTreeClassifier()\n",
    "clfDT = clfDT.fit(xtrain, ctrain.ravel())\n",
    "predictedDT = clfDT.predict(xtest)\n",
    "print_to_file(predictedDT,'testing_kaggle/decision_tree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 1 0 2 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 0 1 1 0 2 1 2\n",
      " 1 2 1 1 1 1 1 1 2 2 1 1 1 2 1 1 0 1 1 2 1 1 1 0 1 1 1 1 0 0 1 1 1 2 2 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 0 1 0 2 2 1 1 1 1 1 1 1 2 1 1 1 2 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 2 1 2 1 1 2 1 1 2 1 1 1 1 1 0 2 2 1 1 1 2 1 1 1 1\n",
      " 0 2 1 1 1 2 0 1 2 1 2 1 0 1 1 2 1 0 1 2 1 1 1 2 1 2 2 2 0 2 1 2 0 1 1 1 1\n",
      " 2 1 1 2 1 1 1 1 1 1 2 1 2 1 2 1 2 1 1 1 2 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1\n",
      " 1 0 1 1 2 2 1 1 1 1 0 1 2 1 2 1 1 1 2 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
      " 1 1 1 0 1 2 1 1 0 1 1 2 1 1 2 0 1 1 2 1 1 1 1 1 0 2 0 1 1 1 1 0 1 2 1 1 1\n",
      " 1 2 1 1 1 1 2 2 2 2 2 1 0 2 1 2 1 1 1 0 2 2 2 1 0 1 1 2 1 1 1 1 2 1 2 2 1\n",
      " 2 1 1 1 0 2 2 1 2 1 0 1 1 0 1 1 1 1 1 1 0 0 2 1 1 1 1 1 0 1 1 0 2 0 1 1 2\n",
      " 1 2 1 1 2 1 1 1 2 1 1 1 1 2 1 2 2 0 0 1 1 0 1 0 0 1 1 2 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 0 2 1 1 0 2 0 0 1 1 1 1 0 1 1 1 2 2 2 1 0 1 1 1 1 1 0 1 0 2\n",
      " 2 1 1 2 2 1 2 2 1 1 1 1 2 2 1 1 2 1 2 2 1 1 2 2 2 0 1 2 1 1 1 0 1 1 0 2 1\n",
      " 1 1 1 2 1 2 2 0 1 1 1 0 1 1 1 1 0 1 2 1 2 1 0 1 1 0 0 2 2 1 2 2 0 2 0 2 1\n",
      " 1 2 1 0 1 0 1 1 1 1 1 1 2 0 1 2 0 1 1 2 1 1 1 1 0 1 1 0 1 1 1 1 1 0 2 2 1\n",
      " 1 1 2 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 2 2 1 2 0 0 1 0 1 2\n",
      " 0 1 2 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    375\n",
       "2    133\n",
       "0     89\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictedDT)\n",
    "pd.value_counts(predictedDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clfNB = MultinomialNB().fit(xtrain, ctrain.ravel())\n",
    "predictedNB = clfNB.predict(xtest)\n",
    "#print_to_file(predictedNB,'testing_kaggle/Naive_bayes_Maruf3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 0 1 0 1 1 1 1 0 2 1 1 0 1 2\n",
      " 1 2 1 1 1 1 1 0 1 1 1 1 1 0 2 1 0 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 2 1 2 1\n",
      " 1 1 1 1 0 1 1 2 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0\n",
      " 1 1 0 1 1 1 1 1 1 0 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 2 1 1 0 1 1 1 2 2 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 0 2 2 1 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 2\n",
      " 2 1 2 2 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 2 1 1 2\n",
      " 1 1 2 1 2 1 1 1 1 1 0 1 1 1 0 1 1 1 1 2 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 2 2 1 1 1 1 1 2 1 2 1 1 1 1 0 1 1 1 0 1 1 1 2 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 2 2 0 1 2 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1\n",
      " 2 1 1 0 0 1 1 0 2 0 1 1 2 1 1 1 1 1 1 1 0 1 1 1 2 1 1 1 1 2 1 0 2 1 1 2 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1\n",
      " 1 1 2 2 1 1 2 2 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 0 1 1 1 1 2 1 1 1\n",
      " 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 2 1 1 1 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 2 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 2 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    472\n",
       "2     90\n",
       "0     35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictedNB)\n",
    "pd.value_counts(predictedNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 2 2\n",
      " 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 0 1 1 1 2\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 2 1 1 1 1 2 1 1 1 1\n",
      " 2 1 2 2 1 1 1 2 1 1 1 1 1 2 1 1 2 1 2 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 2 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1\n",
      " 1 2 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 1 1\n",
      " 2 1 1 1 0 1 1 1 2 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 2 1 0 1 1 1 2 2\n",
      " 2 2 1 1 1 1 1 1 1 1 0 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1\n",
      " 2 2 1 1 1 1 2 2 1 1 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 1 2 1 1 1 2 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 2 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    503\n",
       "2     80\n",
       "0     14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictedNB)\n",
    "pd.value_counts(predictedNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 2 2\n",
      " 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 0 1 1 1 2\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 2 2 2 1 0 1 1 2 1 1 1 1\n",
      " 2 1 2 2 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 2 1 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1\n",
      " 1 2 1 1 1 1 2 1 2 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 1 1\n",
      " 2 1 1 1 0 1 1 1 2 1 1 0 2 1 1 1 1 1 1 1 0 1 1 1 2 1 1 1 1 2 1 0 2 1 1 2 2\n",
      " 2 2 1 1 1 1 1 1 1 1 0 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1\n",
      " 2 2 2 1 1 1 2 2 1 1 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 1 1 2 1 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2\n",
      " 1 1 1 0 1 2 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 2 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    494\n",
       "2     87\n",
       "0     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictedNB)\n",
    "pd.value_counts(predictedNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 50218)\n",
      "(2388, 1)\n",
      "(597, 50218)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "print(training_label.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\n",
      "0  1\n",
      "1  1\n",
      "2  1\n",
      "3  1\n",
      "4  2\n"
     ]
    }
   ],
   "source": [
    "training_label = pd.DataFrame(data = pd.read_csv('assignment3/kaggle/ctrain.csv', header=None))\n",
    "print(training_label.head())\n",
    "# 1    1418\n",
    "# 2     547\n",
    "# 0     423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2388\n",
      "Testing: 597\n"
     ]
    }
   ],
   "source": [
    "print ('Training: ' + str(len(training_data)))\n",
    "print ('Testing: ' + str(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "values not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ff1a982c81c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtraining2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mctrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: values not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 2 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 0 2 1 2 2 0 1\n",
      " 2 2 1 1 1 2 1 1 2 1 1 1 1 2 2 1 2 1 1 1 1 1 1 2 1 1 1 1 0 2 1 0 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 0 1 2 1 1 2 1 1 1 1 1 1 1 2 2 1 1 1 2 1 1 2 2 2 2 1 1 2 2\n",
      " 1 1 0 1 2 1 1 1 2 1 2 2 2 2 2 1 1 2 1 1 2 0 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1\n",
      " 1 1 1 1 1 1 0 1 1 1 2 1 1 2 1 1 2 2 2 1 2 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 0\n",
      " 2 1 2 2 1 1 2 1 1 2 1 1 1 2 2 2 2 1 2 1 2 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 2\n",
      " 1 1 1 2 2 2 1 1 1 1 0 1 2 2 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 1\n",
      " 1 1 0 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 2 2 2 1 1 1 1 2 1 1 1\n",
      " 1 2 1 1 1 1 2 2 2 1 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 1 1 1 1 1 1 1 2 1 2 2 1\n",
      " 2 1 1 1 1 2 1 2 1 2 1 1 1 1 2 1 1 0 1 2 2 2 1 1 1 1 1 2 2 1 1 0 2 1 1 1 2\n",
      " 2 2 2 1 1 1 1 1 2 1 1 1 1 2 2 1 2 1 0 1 1 0 1 1 1 1 1 2 2 2 1 2 2 2 1 1 2\n",
      " 1 1 1 2 1 1 1 2 2 1 1 2 1 2 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 2 1 1 1 1\n",
      " 1 2 1 1 1 2 1 1 2 0 2 1 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 0 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 2 1 1 2 2 2 2 1 1 1 2 1 1 2 1 2 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 2 2 2 1 1 2 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 2 1 2\n",
      " 1 2 1 1 1 2 2 1 1 1 1 1 2 2 2 1 2 0 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2\n",
      " 2 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "training_data = tfidf_transformer.fit_transform(training_data)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "clf_rf = rf.fit(training_data,training_label)\n",
    "y_pred = clf_rf.predict(test_data)\n",
    "print(y_pred)\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# rf = GaussianNB()\n",
    "# clf_rf = rf.fit(training_data,training_label)\n",
    "# print (\"Acurracy: \", clf_rf.score(X_test,y_test))\n",
    "# y_pred = clf_rf.predict(test_data)\n",
    "# print(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn import svm\n",
    "# clf = svm.SVC(class_weight={1: 0.8, 2:0.2, 0: 0.0})\n",
    "# #clf = svm.SVC()\n",
    "# clf.fit(training_data, training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-82dca99269f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# xtrain = np.column_stack((training, training2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# xtest = np.column_stack((test, test2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mxtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# xtrain = np.column_stack((training, training2))\n",
    "# xtest = np.column_stack((test, test2))\n",
    "xtrain = np.column_stack((training_data, training_data2))\n",
    "xtest = np.column_stack((test_data, test_data_2))\n",
    "\n",
    "def print_to_file(output_matrix, name):\n",
    "    with open(name, 'w') as f:\n",
    "        idx = 1\n",
    "        f.write('id,class\\n')\n",
    "        for i in output_matrix:\n",
    "            f.write(str(idx) + ',' +str(i)+'\\n')    \n",
    "            idx += 1\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clfNB = MultinomialNB().fit(xtrain, xtest)\n",
    "predictedNB = clfNB.predict(test_data)\n",
    "print_to_file(predictedNB,'ctestNB.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(predictedNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3e1ec0230416>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Acurracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_rf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \"\"\"\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m    184\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[1;32m--> 380\u001b[1;33m                                       force_all_finite)\n\u001b[0m\u001b[0;32m    381\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \"\"\"\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    244\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "rf = GaussianNB()\n",
    "clf_rf = rf.fit(training_data,training_label)\n",
    "print (\"Acurracy: \", clf_rf.score(X_test,y_test))\n",
    "y_pred = clf_rf.predict(test_data)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\100641313\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-608d22e84c8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf_sv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Acurracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_sv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_sv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "sv = svm.SVC()\n",
    "clf_sv = sv.fit(training_data,training_label)\n",
    "print (\"Acurracy: \", clf_sv.score(X_test,y_test))\n",
    "y_pred = clf_sv.predict(test_data)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3c20a26b47da>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-3c20a26b47da>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    2 2 1 1 0 1 1 1 1 1 1 2 0 1 1 2 2 1 1 1 2 2 1 0 1 2 0 1 1 0 2 1 1 1 2 1 2\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "2 2 1 1 0 1 1 1 1 1 1 2 0 1 1 2 2 1 1 1 2 2 1 0 1 2 0 1 1 0 2 1 1 1 2 1 2\n",
    " 2 1 1 1 2 1 1 1 2 1 1 0 1 1 1 0 1 1 1 1 1 2 2 1 1 0 1 1 1 2 1 1 2 0 2 2 1\n",
    " 1 1 1 1 2 0 2 0 1 1 1 1 1 1 1 1 1 2 1 1 2 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
    " 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 0 1 1 2 0 2 1 1 1 2 1 0 1 1 0 1 1 1 1 1 1\n",
    " 1 1 1 1 2 1 1 1 0 1 1 0 1 1 1 1 1 2 1 0 2 1 2 1 1 1 1 1 0 2 2 1 1 1 1 1 2\n",
    " 1 2 2 1 1 0 1 0 2 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 0 0 2 1 1 0 1 1 1 2 2\n",
    " 1 1 0 2 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 0 1 2 2 1 1 0 1 1\n",
    " 1 1 1 1 1 1 0 0 0 1 2 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1\n",
    " 2 1 1 2 0 1 1 1 1 0 2 1 1 1 1 1 1 2 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 2 1 0 2\n",
    " 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 1 2 1 0 2 1 1 2 1 2\n",
    " 1 1 0 1 1 1 2 1 1 2 2 1 2 1 1 2 0 1 1 1 1 1 1 1 1 1 2 2 0 1 1 1 0 1 1 1 1\n",
    " 1 1 2 1 1 2 0 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1\n",
    " 2 1 2 1 0 0 1 2 0 1 0 1 0 0 1 1 1 2 1 1 1 0 0 2 1 1 2 0 1 1 0 2 1 1 1 1 1\n",
    " 0 1 1 1 1 1 2 1 1 1 0 1 0 1 1 0 1 2 2 0 2 1 1 1 0 1 1 0 0 1 1 1 1 2 1 1 0\n",
    " 1 1 2 1 2 2 1 1 1 0 1 2 1 1 0 1 1 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1\n",
    " 1 1 0 1 1 0 1 1 1 1 0 1 2 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 0 2\n",
    " 1 1 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 1 2 2 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 0 2 1 1 2 2 1 2\n",
    " 1 1 1 1 2 2 2 1 1 1 1 1 2 1 1 2 1 1 2 1 1 0 1 1 1 1 1 0 1 1 1 1 2 1 1 2 1\n",
    " 2 1 1 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 2 1 1 2 2 1 1 1 1\n",
    " 1 1 1 2 1 1 1 2 1 2 1 2 2 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 0 1\n",
    " 2 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 1 1 2\n",
    " 2 2 1 1 1 2 1 1 2 1 1 1 2 2 2 2 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 0 1\n",
    " 1 1 1 2 1 1 1 1 1 2 2 2 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1\n",
    " 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 2 2 2 2 1 1 1 1 2 1 0 1 1\n",
    " 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2\n",
    " 1 1 1 1 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 0 1 2 1 1 1 1 1 2 2 1 0 1 2 1 1 1 2\n",
    " 2 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 0 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 1 1 2 1\n",
    " 2 1 1 1 1 2 2 1 1 1 1 2 1 1 2 1 0 1 1 1 1 2 2 2 1 2 1 1 1 1 2 2 1 1 1 1 1\n",
    " 1 1 1 1 1 1 1 2 0 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 0 1 2 1 1 1 2 2 1 1 1 1 1\n",
    " 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 1 1 2 1 2 2 2 0 2 2 1 1 1\n",
    " 1 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 2 1\n",
    " 1 1 1 1 2 2 0 1 1 1 1 2 1 1 1 2 0 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2\n",
    " 1 1 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.savetxt(\"kaggle_rss2.csv\", y_pred, header=\"class\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ngram_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "unigram_log_pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('logreg', linear_model.LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"ngram\", ngram_pipe),\n",
    "    (\"unigram\", unigram_log_pipe),\n",
    "]\n",
    "\n",
    "mixed_pipe = Pipeline([\n",
    "    (\"voting\", VotingClassifier(classifiers, voting=\"soft\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(data = pd.read_csv('assignment3/kaggle/training.csv', header=None))\n",
    "#training_data2 =pd.DataFrame(data = pd.read_csv('assignment3/kaggle/training2.csv', header=None))\n",
    "training_label = pd.DataFrame(data = pd.read_csv('assignment3/kaggle/ctrain.csv', header=None))\n",
    "test_data = pd.DataFrame(data = pd.read_csv('assignment3/kaggle/test.csv', header=None))\n",
    "#test_data2 = pd.DataFrame(data = pd.read_csv('assignment3/kaggle/test2.csv', header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "Y_COLUMN = \"author\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "\n",
    "\n",
    "def test_pipeline(df, nlp_pipeline):\n",
    "    y = df[Y_COLUMN].copy()\n",
    "    X = pd.Series(df[TEXT_COLUMN])\n",
    "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        nlp_pipeline.fit(X_train, y_train)\n",
    "        losses.append(metrics.log_loss(y_test, nlp_pipeline.predict_proba(X_test)))\n",
    "        accuracies.append(metrics.accuracy_score(y_test, nlp_pipeline.predict(X_test)))\n",
    "\n",
    "    print(\"{kfolds log losses: {0}, mean log loss: {1}, mean accuracy: {2}\".format(\n",
    "        str([str(round(x, 3)) for x in sorted(losses)]),\n",
    "        round(np.mean(losses), 3),\n",
    "        round(np.mean(accuracies), 3)\n",
    "    ))\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\", usecols=[Y_COLUMN, TEXT_COLUMN])\n",
    "test_pipeline(train_df, mixed_pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
